{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb30a5af-5ba8-4928-8a24-11f26111fdbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Ingest circuits.cs file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a597690a-84e7-49a2-9987-1872df1b42dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "circuits_schema = StructType(fields=[StructField('circuitId', IntegerType(), False),\n",
    "                                     StructField('circuitRef', StringType(), True),\n",
    "                                     StructField('name', StringType(), True),\n",
    "                                     StructField('location', StringType(), True),\n",
    "                                     StructField('country', StringType(), True),\n",
    "                                     StructField('lat', DoubleType(), True),\n",
    "                                     StructField('lng', DoubleType(), True),\n",
    "                                     StructField('alt', IntegerType(), True),\n",
    "                                     StructField('url', StringType(), True)\n",
    "                                     \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b39e87ea-ac1a-440b-8371-d3796a5fd989",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "circuits_df = spark.read.schema(circuits_schema).csv('/mnt/udemy/Raw/circuits.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99fbfd80-ba9a-411e-86ef-b3cfb6163d20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[circuitId: int, circuitRef: string, name: string, location: string, country: string, lat: double, lng: double, alt: int, url: string]>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(circuits_df.printSchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4f64e14-9f45-47bf-8475-aa85511f22d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "#### Selecting columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "040c934e-2d9b-4d20-b245-c30eb7c59755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#circuits_selected_df = circuits_df.select('circuitId', 'circuitRef', 'name', 'location', 'country',\n",
    "                                           #'lat', 'lng', 'alt', 'url')\n",
    "# or \n",
    "#circuits_selected_df = circuits_df.select(circuits_df.circuitId, circuits_df.circuitRef, \n",
    "                                          # circuits_df.name, circuits_df.location, circuits_df.country, circuits_df.lat, circuits_df.lng, circuits_df.alt, circuits_df.url)\n",
    "# or\n",
    "#circuits_selected = circuilts_df.select(circuits_df[\"circuitId\"], circuits_df[\"circuitRef\"],\n",
    "                                      #      circuits_df[\"name\"], circuits_df[\"location\"], circuits_df[\"country\"], circuits_df[\"lat\"], circuits_df[\"lng\"], circuits_df[\"alt\"], circuits_df[\"url\"])\n",
    "# or \n",
    "from pyspark.sql.functions import col\n",
    "circuits_selected_df = circuits_df.select(col(\"circuitId\"), col(\"circuitRef\"), col(\"name\"), col(\"location\"),\n",
    "                                           col(\"country\"), col(\"lat\"), col(\"lng\"), col(\"alt\"), col(\"url\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c75d1ad-370e-4ad4-901a-7b2a21b96332",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "circuits_renamed_df = circuits_selected_df.withColumnRenamed(\"circuitId\", \"id\") \\\n",
    "                                          .withColumnRenamed(\"circuitRef\", \"ref\") \\\n",
    "                                          .withColumnRenamed(\"lat\", \"latitude\") \\\n",
    "                                          .withColumnRenamed(\"lng\", \"longitude\") \\\n",
    "                                          .withColumnRenamed(\"alt\", \"altitude\") \\\n",
    "                                          .withColumnRenamed(\"circuitRef\", \"ref\") \\\n",
    "                                          .withColumnRenamed(\"lat\", \"latitude\") \\\n",
    "                                          .withColumnRenamed(\"lng\", \"longitude\") \\\n",
    "                                          .withColumnRenamed(\"alt\", \"altitude\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1502117-1a7c-4e20-8c0c-7e69447610d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "circuits_final_df = circuits_renamed_df.withColumn('ingestion_date', current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "151cd3c1-9a46-4a88-a714-a304c9806d06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Write to datalake using parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83a3c86a-f56e-488f-8a70-1fde1806f33e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/udemy/processed/circuits/</td><td>circuits/</td><td>0</td><td>1736216611000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/udemy/processed/circuits/",
         "circuits/",
         0,
         1736216611000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls('mnt/udemy/processed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c549848-ab2a-49f5-b00e-c7b188f6ce0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "'mnt/udemy/processed/circuits'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "circuits_final_df.write.mode('overwrite').parquet('mnt/udemy/processed/circuits')\n",
    "display('mnt/udemy/processed/circuits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71bceb3b-ca1e-4f59-b374-0c433f121f89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### combine two columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb427773-61e4-450b-9c93-3b4f70d3a65c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import current_timestamp, to_timestamp, concat, col, lit\n",
    "\n",
    "races_schema = StructType(fields=[StructField('raceId', IntegerType(), False),\n",
    "                                  StructField('year', IntegerType(), True),\n",
    "                                  StructField('round', IntegerType(), True),\n",
    "                                  StructField('circuitId', IntegerType(), True),\n",
    "                                  StructField('name', StringType(), True),\n",
    "                                  StructField('date', StringType(), True),\n",
    "                                  StructField('time', StringType(), True),\n",
    "                                  StructField('url', StringType(), True),                                  \n",
    "                                  ])\n",
    "\n",
    "races_df = spark.read.csv('/mnt/udemy/Raw/races.csv', header=True, schema=races_schema)\n",
    "\n",
    "# Add ingestion date and race_timestamp to dataframe\n",
    "races_df = races_df.withColumn('ingestion_date', current_timestamp()) \\\n",
    "                   .withColumn('race_timestamp', to_timestamp(concat(col('date'), lit(' '), col('time')), 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "# Select only the columns required\n",
    "races_selected_df = races_df.select(col('raceId').alias('race_id'),\n",
    "                                     col('year').alias('race_year'),\n",
    "                                     col('round'),\n",
    "                                     col('circuitId').alias('circuit_id'),\n",
    "                                     col('name'),\n",
    "                                     col('ingestion_date'),\n",
    "                                     col('race_timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02cddc40-a369-49f6-a485-a7de4d45297d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 1058\n"
     ]
    }
   ],
   "source": [
    "# Write the data\n",
    "races_selected_df.write.mode('overwrite').partitionBy('race_year').parquet('/mnt/udemy/processed/races')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "651d4ad7-99c9-454d-8921-9d174765c4ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 1058\n"
     ]
    }
   ],
   "source": [
    "written_df = spark.read.parquet('/mnt/udemy/processed/races')\n",
    "print(f\"Number of records: {written_df.count()}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1. Ingest circuits file",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}